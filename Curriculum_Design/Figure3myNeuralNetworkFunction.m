function [Y,Xf,Af] = Figure3myNeuralNetworkFunction(X,~,~)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 08-Jun-2017 13:30:06.
%
% [Y] = myNeuralNetworkFunction(X,~,~) takes these arguments:
%
%   X = 1xTS cell, 1 inputs over TS timesteps
%   Each X{1,ts} = 9xQ matrix, input #1 at timestep ts.
%
% and returns:
%   Y = 1xTS cell of 1 outputs over TS timesteps.
%   Each Y{1,ts} = 2xQ matrix, output #1 at timestep ts.
%
% where Q is number of samples (or series) and TS is the number of timesteps.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [1.51115;10.73;0;0.29;69.81;0;5.43;0;0];
x1_step1.gain = [87.7963125548726;0.300751879699248;0.44543429844098;0.623052959501558;0.357142857142857;0.322061191626409;0.185873605947955;0.634920634920635;3.92156862745098];
x1_step1.ymin = -1;

% Layer 1
b1 = [-1.4505196582398243;0.56076261917120307;-0.26240428660161036;-1.6290993083426839];
IW1_1 = [0.52417136723188107 0.86684851171060762 0.22280577189131001 0.69357501924207765 0.37489214618898153 -0.90659225091563089 -0.31991292879152483 0.2171164372789616 -0.89762956350184342;-0.20868743233442755 -0.70843633208958379 0.30926993678202347 -0.88278274503087206 -0.59263434046342744 -0.54708723743812449 1.1457006709037629 -0.098048087569543363 1.0143489586855843;1.0830926993167895 -0.88272747755811576 2.9497748061254834 -1.4345799630328264 0.15083135583799587 0.027687956708200748 0.062782351192921221 0.3327571176288277 0.43876635277204684;-0.46670331360121703 0.66673189398641808 0.55896462588936535 0.46201568263753745 0.67815443648943685 0.48694654913643826 0.70528340761833419 -0.11990319113043701 0.51560650306399436];

% Layer 2
b2 = [0.40764427075081999;0.79906790133227712];
LW2_1 = [0.87309249679613177 -1.0752296872322873 -1.4088038648599595 -0.33736800409751772;-0.11779741014421044 2.0408919812667805 2.048895881396898 -0.67504991019083305];

% ===== SIMULATION ========

% Format Input Arguments
isCellX = iscell(X);
if ~isCellX, X = {X}; end;

% Dimensions
TS = size(X,2); % timesteps
if ~isempty(X)
    Q = size(X{1},2); % samples/series
else
    Q = 0;
end

% Allocate Outputs
Y = cell(1,TS);

% Time loop
for ts=1:TS
    
    % Input 1
    Xp1 = mapminmax_apply(X{1,ts},x1_step1);
    
    % Layer 1
    a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*Xp1);
    
    % Layer 2
    a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);
    
    % Output 1
    Y{1,ts} = a2;
end

% Final Delay States
Xf = cell(1,0);
Af = cell(2,0);

% Format Output Arguments
if ~isCellX, Y = cell2mat(Y); end
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings)
y = bsxfun(@minus,x,settings.xoffset);
y = bsxfun(@times,y,settings.gain);
y = bsxfun(@plus,y,settings.ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n,~)
if isa(n,'gpuArray')
    a = iSoftmaxApplyGPU(n);
else
    a = iSoftmaxApplyCPU(n);
end
end
function a = iSoftmaxApplyCPU(n)
nmax = max(n,[],1);
n = bsxfun(@minus,n,nmax);
numerator = exp(n);
denominator = sum(numerator,1);
denominator(denominator == 0) = 1;
a = bsxfun(@rdivide,numerator,denominator);
end
function a = iSoftmaxApplyGPU(n)
nmax = max(n,[],1);
numerator = arrayfun(@iSoftmaxApplyGPUHelper1,n,nmax);
denominator = sum(numerator,1);
a = arrayfun(@iSoftmaxApplyGPUHelper2,numerator,denominator);
end
function numerator = iSoftmaxApplyGPUHelper1(n,nmax)
numerator = exp(n - nmax);
end
function a = iSoftmaxApplyGPUHelper2(numerator,denominator)
if (denominator == 0)
    a = numerator;
else
    a = numerator ./ denominator;
end
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n,~)
a = 2 ./ (1 + exp(-2*n)) - 1;
end
